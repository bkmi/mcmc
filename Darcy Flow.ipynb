{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darcy Flow\n",
    "Including forward and inverse problem solver for 1D. Darcy Flow\n",
    "Darcy Flow\n",
    "\\begin{equation}\n",
    "    \\nabla \\cdot ( k(x) \\nabla u(x) ) = f(x)\n",
    "\\end{equation}\n",
    "\n",
    "Zero Dirichlet boundary conditions $u(0) = u(1) = 0$\n",
    "\n",
    "## Hints from Tim Sullivan\n",
    "\n",
    "Express $u$ as $u(x) = \\sum_{j = 1}^{n} u_{j} \\phi_{j}(x)$ with $\\phi_{j}(x) =$ piecewise linear tent function peaking at node $j$\n",
    "\n",
    "Solve $A(u_{1}, …, u_{n}) = b$ for the coefficients  \n",
    "\\begin{align}\n",
    "b_{j} &= \\int_{0}^{1} f(x) \\phi_{j}(x) d x \\\\\n",
    "A_{i j} &= \\int_{0}^{1} k(x) \\phi'_{i}(x) \\phi'_{j}(x) d x\n",
    "\\end{align}\n",
    "Google finite element method / Galerkin method for elliptic PDE \n",
    "\n",
    "I will give you e.g. (u(1/4), u(1/2), u(3/4)) and similarly for f, both corrupted by additive N(0, \\sigma^{2}) noise.\n",
    "Your challenge:  infer k\n",
    "\n",
    "--\n",
    "\n",
    "Modelling assumption:  $k(x) = exp( \\sum_{\\alpha = 0}^{A} k_{\\alpha} \\phi_{\\alpha}(x) )$\n",
    "\n",
    "--\n",
    "\n",
    "Try a Fourier basis. Note that u vanishes at the boundary, but that doesn't mean k does.\n",
    "\n",
    "Create the forward model. Solver works. Extend to imitation inverse model with likelihood, etc. Use this to solve his problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Darcy Flow Problem\n",
    "\n",
    "We consider the Darcy Flow problem in one dimension with Dirichlet boundry conditions and a modeling assumption.\n",
    "\n",
    "\\begin{align}\n",
    "    k'(x) u'(x) + k(x) u''(x) &= f(x) \\\\\n",
    "    u(0) = u(1) &= 0 \\\\\n",
    "    exp( \\sum_{\\alpha = 0}^{A} k_{\\alpha} \\phi_{\\alpha}(x) ) &=  k(x)\n",
    "\\end{align}\n",
    "\n",
    "We put the equation into the weak form. $v(x)$ is a function which satisfies the boundary conditions.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\int_{0}^{1} f(x)v(x) \\ dx = \\int_{0}^{1} (k'(x) u'(x) v(x) + k(x) u''(x) v(x)) \\ dx\n",
    "\\end{equation}\n",
    "\n",
    "We can integrate by parts and apply our boundry conditions on the second R.H.S. term to arrive at the next equation.\n",
    "\n",
    "\\begin{align}\n",
    "    \\int_{0}^{1} f(x)v(x) \\ dx &= \\int_{0}^{1} k'(x) u'(x) v(x) \\ dx +\n",
    "                                  k(x) u'(x) v(x) |_{0}^{1} - \n",
    "                                  \\int_{0}^{1} k'(x) u'(x) v(x) \\ dx - \n",
    "                                  \\int_{0}^{1} k(x) u'(x) v'(x) \\ dx \\\\\n",
    "    \\int_{0}^{1} f(x)v(x) \\ dx &= - \\int_{0}^{1} k(x) u'(x) v'(x) \\ dx\n",
    "\\end{align}\n",
    "\n",
    "We choose the piecewise linear function $v_k$ for a discretization.\n",
    "\n",
    "\\begin{equation}\n",
    "v_{k}(x) = \n",
    "    \\begin{cases}\n",
    "        \\frac{x-x_{k-1}}{x_k-x_{k-1}} & \\text{if } x \\in [x_{k-1}, x_k] \\\\\n",
    "        \\frac{x_{k+1}-x}{x_{k+1}-x_k} & \\text{if } x \\in [x_{k}, x_{k+1}] \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "If we expand $u(x)$ in a basis of tent functions on this discretization, we are left with the problem\n",
    "\n",
    "\\begin{equation}\n",
    "    A \\bf{u} = \\bf{b}\n",
    "\\end{equation}\n",
    "\n",
    "where $A_{ij} = - \\int_{0}^{1} k(x) v_{i}'(x) v_{j}'(x) \\ dx$ and $b_{j} = \\int_{0}^{1} f(x) v_{j} dx$. Note that this will be a sparse matrix due our use of tent functions. With our modeling assumption, only \n",
    "\n",
    "## Useful Links\n",
    "https://en.wikipedia.org/wiki/Finite_element_method  \n",
    "http://www.mathematik.uni-dortmund.de/~kuzmin/Transport.pdf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Forward Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid(nodes):\n",
    "    return np.linspace(0, 1, num=nodes)\n",
    "    \n",
    "def tent(x, k, grid):\n",
    "    def down(x):\n",
    "        return (grid[k+1] - x) / (grid[k+1] - grid[k])\n",
    "    def up(x):\n",
    "        return (x - grid[k-1]) / (grid[k] - grid[k-1])\n",
    "    if (k < 0) or (k > grid.size - 1):\n",
    "        raise ValueError('k was not in [0, grid.size - 1]')\n",
    "    elif k == 0:\n",
    "        if (grid[k] <= x) and (x <= grid[k+1]):\n",
    "            return down(x)\n",
    "        else:\n",
    "            return 0\n",
    "    elif k == grid.size - 1:\n",
    "        if (grid[k-1] <= x) and (x <= grid[k]):\n",
    "            return up(x)\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if (grid[k-1] <= x) and (x <= grid[k]):\n",
    "            return up(x)\n",
    "        elif (grid[k] <= x) and (x <= grid[k+1]):\n",
    "            return down(x)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def κ(x, coefs, grid):\n",
    "    lo_bound = np.searchsorted(grid, x, 'left')\n",
    "    up_bound = np.searchsorted(grid, x, 'right')\n",
    "    \n",
    "    if (0 - 0.1 <= x) and (x < grid[1]):\n",
    "        return np.exp(sum([coefs[0] * tent(x, 0, grid), coefs[1] * tent(x, 1, grid)]))\n",
    "    elif (grid[-2] < x) and (x <= grid[-1] + 0.1):\n",
    "        return np.exp(sum([coefs[-2] * tent(x, grid.size - 2, grid), coefs[-1] * tent(x, grid.size - 1, grid)]))\n",
    "    else:\n",
    "        return np.exp(sum([coefs[k] * tent(x, k, grid) for k in [lo_bound, up_bound]]))\n",
    "    \n",
    "# def kappa(x, coefs, grid):\n",
    "#     return np.exp(sum([coefs[k] * tent(x, k, grid) for k in range(grid.size)]))\n",
    "\n",
    "def A(k, grid):\n",
    "    val = lambda x, y: sp.integrate.quad(k, x, y, limit=100)[0]\n",
    "    lo_di = np.asarray([ val(grid[i-1], grid[i]  ) for i in range(2, grid.size - 1)])\n",
    "    di    = np.asarray([-val(grid[i-1], grid[i+1]) for i in range(1, grid.size - 1)])\n",
    "    up_di = np.asarray([ val(grid[i]  , grid[i+1]) for i in range(1, grid.size - 2)])\n",
    "    \n",
    "    return np.sum([np.diag(lo_di, -1), np.diag(di), np.diag(up_di, 1)], axis=0)\n",
    "\n",
    "def b(f, grid):\n",
    "    val = lambda x, y: sp.integrate.quad(f, x, y)[0]\n",
    "    return np.asarray([-val(grid[i-1], grid[i+1]) for i in range(1, grid.size - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes = 5\n",
    "g = grid(nodes)\n",
    "plt.plot([κ(x, np.random.rand(nodes), g) for x in np.linspace(0, 1, num=100)])\n",
    "\n",
    "for i in range(nodes):\n",
    "    plt.plot([tent(x, i, g) for x in np.linspace(0, 1, num=100)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes = 10\n",
    "pars = np.random.randn(nodes)\n",
    "g = grid(nodes)\n",
    "aa = A(lambda x: κ(x, pars, g), g)\n",
    "bb = b(lambda x: np.cos(x), g)\n",
    "\n",
    "np.linalg.solve(aa, bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inverse Problem\n",
    "\n",
    "Given our Darcy Flow system, the coefficents of $u_{i}(x)$, and samples from $f(x)$, both corrupted by additive noise $\\mathcal{N}(0, \\sigma^{2})$, it is our job to infer the paramters $k_{\\alpha}$. The additive noise implies that \n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{u} &\\rightarrow \\mathbf{u} + \\mathcal{N}(0, \\sigma^{2}) \\\\\n",
    "    \\mathbf{b_j} &\\rightarrow \\int_{0}^{1} (f(x) + \\mathcal{N}(0, \\sigma^{2}))  v_{j} \\ dx. \n",
    "\\end{align}\n",
    "\n",
    "This yeilds the system\n",
    "\n",
    "\n",
    "We restate our deterministic system $A \\mathbf{u} = \\mathbf{b}$. \n",
    "\n",
    "\n",
    "The noise does not depend on the integration parameters in $A_{ij}$ or $b_{j}$, therefore we are left with the system \n",
    "\n",
    "\\begin{equation}\n",
    "    A \\mathbf{u}  = \\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "I will give you e.g. (u(1/4), u(1/2), u(3/4)) and similarly for f, both corrupted by additive N(0, \\sigma^{2}) noise.\n",
    "Your challenge:  infer k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from sampled import sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(A):\n",
    "    \n",
    "    def G(u):\n",
    "        \n",
    "    return g\n",
    "\n",
    "def tt_posterior(G, y, gamma, sigma):\n",
    "    \n",
    "    def logp(u):\n",
    "        - tt.dot((y - G(u)).T, y - G(u))/(2 * gamma ** 2) - tt.dot(u.T, u)/(2 * sigma ** 2)\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tt_banana_pdf(mean, cov, warp):\n",
    "    mean = np.asarray(mean)\n",
    "    cov = np.asarray(cov)\n",
    "    dim = mean.shape[0]\n",
    "    \n",
    "    constant = -np.log((2*np.pi)**dim * np.linalg.det(cov))/2\n",
    "    covinv = np.linalg.inv(cov)\n",
    "    \n",
    "    def logp(x):\n",
    "        distortion = np.ones(dim) * warp * x[0]**2\n",
    "        tt.set_subtensor(distortion[0], 0)\n",
    "        return constant - tt.dot(tt.dot((x + distortion - mean).T, covinv), (x + distortion - mean))/2\n",
    "    return logp\n",
    "\n",
    "@sampled\n",
    "def banana(mean=[0,0], cov=[[1,0],[0,1]], warp=0.9, **observed):\n",
    "    mean = np.asarray(mean)\n",
    "    cov = np.asarray(cov)\n",
    "    dim = mean.shape[0]\n",
    "    testval = np.zeros(dim)\n",
    "    pm.DensityDist('banana', logp=tt_banana_pdf(mean, cov, warp), shape=dim, testval=testval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 2\n",
    "mean = np.zeros(dim)\n",
    "cov = np.eye(dim)/dim\n",
    "warp = 0.5\n",
    "\n",
    "starting_point = np.zeros(dim)\n",
    "\n",
    "with banana(mean=mean, cov=cov, warp=warp):\n",
    "    step = pm.Metropolis()\n",
    "    step = pm.NUTS()\n",
    "    metropolis_sample = pm.sample(draws=1000, step=step, \n",
    "                                  start={'banana': starting_point}, \n",
    "                                  tune=500, discard_tuned_samples=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
