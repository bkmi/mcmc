{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import distributions as dist\n",
    "\n",
    "from metropolis import acceptance_rate_per_step, conditional_prob, mala_step, lagk_ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Setup\n",
    "\n",
    "We have a target distribution and we will try to sample from it using Metropolis-Hastings MCMC with MALA proposal technqiues using various proposal distributions.\n",
    "\n",
    "Some useful links were found here:  \n",
    "https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm  \n",
    "http://www.mcmchandbook.net/HandbookChapter1.pdf  \n",
    "https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/  \n",
    "\n",
    "Information on lag-k autocorrelation:\n",
    "http://www.itl.nist.gov/div898/handbook/eda/section3/eda35c.htm\n",
    "\n",
    "Given measurements, $Y_1, Y_2, ..., Y_N$ at time $X_1, X_2, ..., X_N$, the lag k autocorrelation function is defined as\n",
    "\\begin{equation}\n",
    "    r_k = \\frac{\\sum_{i=1}^{N-k}(Y_{i} - \\bar{Y})(Y_{i+k} - \\bar{Y})}{\\sum_{i=1}^{N}(Y_{i} - \\bar{Y})^{2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select bimodal\n",
    "if False:\n",
    "    mean1 = [2, 0]\n",
    "    cov1 = [[1, 0], [0, 1]]\n",
    "\n",
    "    mean2 = [-2, 0]\n",
    "    cov2 = [[1, 0], [0, 1]]\n",
    "\n",
    "    tau = 1.0\n",
    "    \n",
    "    rv = dist.bimodal_dist(mean1, mean2, cov1, cov2)\n",
    "    \n",
    "# Select parabolic gaussian\n",
    "if True:\n",
    "    mean = [0, 0]\n",
    "    cov = [[1, 0], [0, 1]]\n",
    "    \n",
    "    tau = 0.4\n",
    "\n",
    "    rv = dist.parabolic_gaussian(mean, cov, warp=0.9)\n",
    "    \n",
    "# Select streched gaussian\n",
    "if False:\n",
    "    mean = [0, 0]\n",
    "    cov = [[1, 0], [0, 100]]\n",
    "    \n",
    "    tau = 1.0\n",
    "    \n",
    "    rv = dist.stretched_normal(mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "ax1.plot(*rv.rvs(1000).T, marker='o', linestyle='None')\n",
    "ax1.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "x, y = np.mgrid[-10:10:.1,\n",
    "                -10:10:.1]\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "# pdf\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.contourf(x, y, rv.pdf(pos))\n",
    "plt.show()\n",
    "\n",
    "# logpdf\n",
    "fig3 = plt.figure()\n",
    "ax3 = fig3.add_subplot(111)\n",
    "ax3.contourf(x, y, rv.logpdf(pos))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magnitude of gradpdf\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.contourf(x, y, np.linalg.norm(rv.grad_pdf(pos), axis=0))\n",
    "plt.show()\n",
    "\n",
    "# magnitude of grad log pdf\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.contourf(x, y, np.linalg.norm(rv.grad_logpdf(pos), axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MALA Proposal\n",
    "\n",
    "We propose by suggesting the state plus the gradient of the target distribution at the state plus a small random motion.\n",
    "\n",
    "Note, for reference, the derivate with respect to the state of our target distribution is given by:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\pi(\\mathbf{x})}{\\partial\\mathbf{x}}=-\\frac{1}{\\sqrt{det(2\\pi\\mathbf{\\Sigma})}}exp\\left[-\\frac{1}{2}(\\mathbf{x}-\\mathbf{m})^{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{m})\\right]\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{m})\n",
    "\\end{equation}\n",
    "\n",
    "However, this is a bit irrelevant since I am looking for the gradient at the state. I.e. $\\nabla\\pi(\\textbf{x})$ I will do it numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC Execution\n",
    "states = [np.array([0,0])]\n",
    "steps = 1000\n",
    "\n",
    "import scipy.stats\n",
    "proposal_dist = [scipy.stats.multivariate_normal([0, 0], np.eye(2)),\n",
    "                scipy.stats.multivariate_normal([0, 0], np.asarray([[1,0],[0,10]]))]\n",
    "\n",
    "for i in range(steps):\n",
    "    states.append(mala_step(states[-1], rv, v=False, log=True, tau=tau, proposal_dist=proposal_dist))\n",
    "    \n",
    "states = np.asarray(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceptance rate plot\n",
    "acps = acceptance_rate_per_step(states)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(steps), acps)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Acceptance Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and trace plots\n",
    "df = pd.DataFrame(states, columns=[\"x\", \"y\"])\n",
    "\n",
    "plt.figure(1, figsize=(8,8))\n",
    "plt.subplot(121)\n",
    "plt.scatter(df.x, df.y, c=rv.pdf(states))\n",
    "plt.axis('equal')\n",
    "plt.title('Samples plot')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(np.arange(states.shape[0]), df.x)\n",
    "plt.title('Trace plot')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('x')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(np.arange(states.shape[0]), df.y)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def lag_line(sequence, k):\n",
    "#     plot_ac = []\n",
    "#     variance = np.sum(np.dot(sequence - np.mean(sequence, axis=0), (sequence - np.mean(sequence, axis=0)).T).diagonal())\n",
    "\n",
    "#     for i in range(len(sequence)):\n",
    "#         if i < k:\n",
    "#             pass\n",
    "#         else:\n",
    "#             plot_ac.append([i, lagk_ac(sequence[:i], k, v=False, variance=variance)])\n",
    "#     return np.asarray(plot_ac)\n",
    "\n",
    "# lag1 = lag_line(states, 1)\n",
    "# lag5 = lag_line(states, 5)\n",
    "# lag10 = lag_line(states, 10)\n",
    "# lag50 = lag_line(states, 50)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.plot(lag1[:,0], lag1[:,1])\n",
    "# ax.plot(lag5[:,0], lag5[:,1])\n",
    "# ax.plot(lag10[:,0], lag10[:,1])\n",
    "# ax.plot(lag50[:,0], lag50[:,1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr(x):\n",
    "    \"\"\"\n",
    "    Computes the ( normalised) auto-correlation function of a\n",
    "    one dimensional sequence of numbers.\n",
    "    \n",
    "    Utilises the numpy correlate function that is based on an efficient\n",
    "    convolution implementation.\n",
    "    \n",
    "    Inputs:\n",
    "    x - one dimensional numpy array\n",
    "    \n",
    "    Outputs:\n",
    "    Vector of autocorrelation values for a lag from zero to max possible\n",
    "    \"\"\"\n",
    "    \n",
    "    # normalise, compute norm\n",
    "    xunbiased = x - np.mean(x)\n",
    "    xnorm = np.sum(xunbiased ** 2)\n",
    "    \n",
    "    # convolve with itself\n",
    "    acor = np.correlate(xunbiased, xunbiased, mode='same')\n",
    "    \n",
    "    # use only second half, normalise\n",
    "    acor = acor[int(len(acor) / 2):] / xnorm\n",
    "    \n",
    "    return acor\n",
    "\n",
    "\n",
    "y = autocorr(states[:,0])\n",
    "\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
